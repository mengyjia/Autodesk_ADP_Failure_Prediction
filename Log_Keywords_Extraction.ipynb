{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare logs to neural network.**\n",
    "\n",
    "I will follow the steps belowï¼š\n",
    "\n",
    "1. Import packages and keywords\n",
    "\n",
    "2. Load log files\n",
    "\n",
    "3. Transforming keywords to integers\n",
    "\n",
    "4. Extract keyword information from logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "from itertools import chain\n",
    "\n",
    "import boto3\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>java.lang.InterruptedException</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NoSuchObjectException</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AddressNotFoundException</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>java.io.IOException</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>java.net.SocketException</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          keyword\n",
       "0  java.lang.InterruptedException\n",
       "1           NoSuchObjectException\n",
       "2        AddressNotFoundException\n",
       "3             java.io.IOException\n",
       "4        java.net.SocketException"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load keywords from csv\n",
    "keywords = pd.read_csv('keywords.csv')\n",
    "keywords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file name of logs from S3\n",
    "client = boto3.client('s3') # low-level functional API\n",
    "resource = boto3.resource('s3') # high-level object-oriented API\n",
    "my_bucket = resource.Bucket('adp.spark.app.logs.us-west-1.prd') # subsitute this for your s3 bucket name.\n",
    "files = list(my_bucket.objects.filter(Prefix='logs/aggregates')) # put the log files's path and name into a list\n",
    "\n",
    "# load log content using PySpark\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"local\")\n",
    "         .setAppName(\"My app\") # used to be Simple App\n",
    "         .set(\"spark.driver.maxResultSize\", \"2g\"))\n",
    "\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of log files : 108382\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of log files : {}\".format(len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract file name and file content to filename and filecontent, respectively\n",
    "filename = []\n",
    "filecontent = []\n",
    "\n",
    "def append_func(files):\n",
    "    global filename\n",
    "    global filecontent\n",
    "    \n",
    "    filename = []\n",
    "    filecontent = []\n",
    "    for file in files:\n",
    "        if 'big.data.services.team.log' in file.key:\n",
    "            filename.append(file.key.replace('logs/aggregates/','').replace('_asrd.cp.big.data.services.team.log',''))\n",
    "            file_content = sc.textFile('s3n://adp.spark.app.logs.us-west-1.prd/' + file.key).collect()\n",
    "            filecontent.append(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append_func(files)\n",
    "#print(f'filename: {len(filename)}')\n",
    "#print(f'filecontent: {len(filecontent)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming keywords to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform keywords to integer labels\n",
    "keywords['label'] = np.array(keywords.index)+1\n",
    "keywords.label = keywords.label.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>java.lang.InterruptedException</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NoSuchObjectException</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AddressNotFoundException</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>java.io.IOException</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>java.net.SocketException</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          keyword label\n",
       "0  java.lang.InterruptedException     1\n",
       "1           NoSuchObjectException     2\n",
       "2        AddressNotFoundException     3\n",
       "3             java.io.IOException     4\n",
       "4        java.net.SocketException     5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract keyword information from logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match keywords that appear in one line of one log file\n",
    "def match_keywords(line):\n",
    "    for index, value in enumerate(list(keywords.keyword)):\n",
    "        line = line.replace(value,'$' + keywords.label[index] + '$')\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract keywords from logs and stored in nested lists, encode keyword to $label$\n",
    "def extract_keywords(filecontent, keywords):\n",
    "\n",
    "    def file_extract(filecontent_file):\n",
    "        keyword_in_file = list(map(lambda x: re.findall(r'\\$\\d+\\$', match_keywords(x)), filecontent_file))\n",
    "        return keyword_in_file\n",
    "    \n",
    "    keyword_in_file_all = list(map(file_extract ,filecontent))\n",
    "    \n",
    "    return keyword_in_file_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlist results generated from extract_keywords, and decode $label$ to label\n",
    "def unlist(keyword_in_file_all):\n",
    "    \n",
    "    def unnest_list(x):\n",
    "        sub_list = list(chain.from_iterable(x))\n",
    "        def decode(x):\n",
    "            decoded = x.replace('$','')\n",
    "            return decoded\n",
    "        word_list = list(map(decode, sub_list))\n",
    "        return word_list\n",
    "\n",
    "    unlist_keywords = list(map(unnest_list, keyword_in_file_all))\n",
    "        \n",
    "    # assign 0 to those files that does not contain any listing keywords\n",
    "    def assign_zero(x):\n",
    "        if not x:\n",
    "            x = ['0']\n",
    "        return x\n",
    "    \n",
    "    keyword_in_file_all_unlist = list(map(assign_zero ,unlist_keywords))\n",
    "            \n",
    "    return keyword_in_file_all_unlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract log results(fail or success)\n",
    "# assumption1: one log file only contain one result, and this result is only for current log.\n",
    "# assumption2: one log file = one log\n",
    "def extract_results(filecontent):\n",
    "    \n",
    "    def which_result_appear(x):\n",
    "        if (\"SparkSubmit.exceptionExitHook[failure]\" in x):\n",
    "            result = 0\n",
    "        elif (\"SparkSubmit.successfulExitHook[success]\" in x):\n",
    "            result = 1\n",
    "        else:\n",
    "            result = -1\n",
    "        return result\n",
    "    \n",
    "    def file_extract(filecontent_file):\n",
    "        job_result = list(map(which_result_appear, filecontent_file))\n",
    "        if (0 in job_result):\n",
    "            job_result = 0\n",
    "        elif (1 in job_result):\n",
    "            job_result = 1\n",
    "        else:\n",
    "            job_result = -1\n",
    "        return str(job_result)\n",
    "    \n",
    "    keyword_in_file_all = list(map(file_extract ,filecontent))\n",
    "    \n",
    "    return keyword_in_file_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define result variables\n",
    "keyword_in_file_all = []\n",
    "keyword_in_file_all_unlist = []\n",
    "job_result = []\n",
    "file_name = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract keyword information from logs\n",
    "def parsing():\n",
    "    size = np.arange(31532,len(files),1)\n",
    "    for i in range(len(size)):\n",
    "        if i == 2468:\n",
    "            break\n",
    "        sample_files = files[size[i]:size[i+1]]\n",
    "        \n",
    "        append_func(sample_files)\n",
    "    \n",
    "        file_name.append(filename)\n",
    "    \n",
    "        keyword_in_file_all_iter = extract_keywords(filecontent, keywords)\n",
    "        keyword_in_file_all.append(keyword_in_file_all_iter)\n",
    "        \n",
    "        keyword_in_file_all_unlist_iter = unlist(keyword_in_file_all_iter)\n",
    "        keyword_in_file_all_unlist.append(keyword_in_file_all_unlist_iter)\n",
    "        \n",
    "        job_result_iter = extract_results(filecontent)\n",
    "        job_result.append(job_result_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run parsing function\n",
    "parsing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlist result variables\n",
    "keyword_in_file_all = list(chain.from_iterable(keyword_in_file_all))\n",
    "keyword_in_file_all_unlist = list(chain.from_iterable(keyword_in_file_all_unlist))\n",
    "job_result = list(chain.from_iterable(job_result))\n",
    "file_name = list(chain.from_iterable(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name: 2463\n",
      "keyword_in_file_all: 2463\n",
      "keyword_in_file_all_unlist: 2463\n",
      "job_result: 2463\n"
     ]
    }
   ],
   "source": [
    "print(f'file_name: {len(file_name)}')\n",
    "print(f'keyword_in_file_all: {len(keyword_in_file_all)}')\n",
    "print(f'keyword_in_file_all_unlist: {len(keyword_in_file_all_unlist)}')\n",
    "print(f'job_result: {len(job_result)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export file to csv\n",
    "res = job_result\n",
    "csvfile = '/home/ec2-user/UCD_MSBA_team/6Shanxing_Branch/results_in_csv/8job_result2.csv'\n",
    "\n",
    "#Assuming res is a list of lists\n",
    "with open(csvfile, \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in res:\n",
    "        writer.writerow([val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export file to csv\n",
    "res = keyword_in_file_all_unlist\n",
    "csvfile = '/home/ec2-user/UCD_MSBA_team/6Shanxing_Branch/results_in_csv/8keyword_in_job2.csv'\n",
    "\n",
    "#Assuming res is a list of lists\n",
    "with open(csvfile, \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export file to csv\n",
    "res = file_name\n",
    "csvfile = '/home/ec2-user/UCD_MSBA_team/6Shanxing_Branch/results_in_csv/8file_name2.csv'\n",
    "\n",
    "#Assuming res is a flat list\n",
    "with open(csvfile, \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in res:\n",
    "        writer.writerow([val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing the algorithm\n",
    "#%timeit parsing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing the algorithm\n",
    "#%timeit extract_keywords(filecontent, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing the algorithm\n",
    "#%timeit unlist(keyword_in_file_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing the algorithm\n",
    "#%timeit extract_results(filecontent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Conclusion]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[description of your conclusion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Reference]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://spark.apache.org/docs/0.9.1/python-programming-guide.html\n",
    "\n",
    "[reference website 2]\n",
    "\n",
    "[reference website 3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
