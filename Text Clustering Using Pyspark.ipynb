{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(\"local\", \"Simple App\")   # Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile('s3n://adp.spark.app.logs.us-west-1.prd/logs/aggregates/*.log')\n",
    "# data is an RDD (spark) =  resilient distributed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['log4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.',\n",
       " '18/01/24 05:18:33 INFO s3OperationsLog: Method=HEAD ResponseCode=404 URI=http://com.autodesk.edl.prd.s3.amazonaws.com/apps%2Fqubole%2Faccount_id%2F4322%2Flogs%2Fhadoop%2F44195%2F759707%2Fapp-logs%2Fasrd.cp.big.data.services.team%2Flogs%2Fapplication_1513303661803_104617',\n",
       " '18/01/24 05:18:33 INFO s3OperationsLog: Method=GET ResponseCode=200 URI=http://com.autodesk.edl.prd.s3.amazonaws.com/?delimiter=%2F&max-keys=1000&prefix=apps%2Fqubole%2Faccount_id%2F4322%2Flogs%2Fhadoop%2F44195%2F759707%2Fapp-logs%2Fasrd.cp.big.data.services.team%2Flogs%2Fapplication_1513303661803_104617%2F',\n",
       " '18/01/24 05:18:33 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum',\n",
       " '18/01/24 05:18:33 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://com.autodesk.edl.prd.s3.amazonaws.com/apps%2Fqubole%2Faccount_id%2F4322%2Flogs%2Fhadoop%2F44195%2F759707%2Fapp-logs%2Fasrd.cp.big.data.services.team%2Flogs%2Fapplication_1513303661803_104617%2Fip-10-47-7-47.us-west-1.compute.internal_45454',\n",
       " '18/01/24 05:18:33 INFO s3OperationsLog: Method=GET ResponseCode=200 URI=http://com.autodesk.edl.prd.s3.amazonaws.com/apps%2Fqubole%2Faccount_id%2F4322%2Flogs%2Fhadoop%2F44195%2F759707%2Fapp-logs%2Fasrd.cp.big.data.services.team%2Flogs%2Fapplication_1513303661803_104617%2Fip-10-47-7-47.us-west-1.compute.internal_45454',\n",
       " '18/01/24 05:18:33 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://com.autodesk.edl.prd.s3.amazonaws.com/apps%2Fqubole%2Faccount_id%2F4322%2Flogs%2Fhadoop%2F44195%2F759707%2Fapp-logs%2Fasrd.cp.big.data.services.team%2Flogs%2Fapplication_1513303661803_104617%2Fip-10-47-7-47.us-west-1.compute.internal_45454',\n",
       " '18/01/24 05:18:33 INFO s3OperationsLog: Method=GET ResponseCode=206 URI=http://com.autodesk.edl.prd.s3.amazonaws.com/apps%2Fqubole%2Faccount_id%2F4322%2Flogs%2Fhadoop%2F44195%2F759707%2Fapp-logs%2Fasrd.cp.big.data.services.team%2Flogs%2Fapplication_1513303661803_104617%2Fip-10-47-7-47.us-west-1.compute.internal_45454',\n",
       " '18/01/24 05:18:33 INFO s3OperationsLog: Method=GET ResponseCode=206 URI=http://com.autodesk.edl.prd.s3.amazonaws.com/apps%2Fqubole%2Faccount_id%2F4322%2Flogs%2Fhadoop%2F44195%2F759707%2Fapp-logs%2Fasrd.cp.big.data.services.team%2Flogs%2Fapplication_1513303661803_104617%2Fip-10-47-7-47.us-west-1.compute.internal_45454',\n",
       " '18/01/24 05:18:33 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = data.sample(False, 0.01, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f = data.filter(lambda line: 'asks yarn to' in line.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18/01/20 14:43:10 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_104618',\n",
       " '18/01/20 14:26:42 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_104736',\n",
       " '18/01/20 15:11:21 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_104848',\n",
       " '18/01/20 14:23:38 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_104831',\n",
       " '18/01/20 15:30:57 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_104930',\n",
       " '18/01/20 15:36:41 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_104977',\n",
       " '18/01/20 15:38:04 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_104987',\n",
       " '18/01/20 15:38:18 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_104992',\n",
       " '18/01/20 15:41:05 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_104994',\n",
       " '18/01/20 15:42:47 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_104996',\n",
       " '18/01/20 15:52:42 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105021',\n",
       " '18/01/20 15:54:14 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105024',\n",
       " '18/01/20 16:01:20 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105034',\n",
       " '18/01/20 16:01:45 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105061',\n",
       " '18/01/20 16:05:18 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105063',\n",
       " '18/01/20 16:03:47 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105070',\n",
       " '18/01/20 16:05:14 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105077',\n",
       " '18/01/20 16:17:05 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105114',\n",
       " '18/01/20 16:31:41 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105125',\n",
       " '18/01/20 16:22:06 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105128',\n",
       " '18/01/20 17:01:27 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105170',\n",
       " '18/01/20 17:01:38 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105176',\n",
       " '18/01/20 17:19:30 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105177',\n",
       " '18/01/20 18:07:11 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105266',\n",
       " '18/01/20 18:12:00 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105269',\n",
       " '18/01/20 19:20:06 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105317',\n",
       " '18/01/20 19:13:34 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105330',\n",
       " '18/01/20 19:13:55 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105334',\n",
       " '18/01/20 19:30:43 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105352',\n",
       " '18/01/20 20:01:25 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105371',\n",
       " '18/01/20 20:05:21 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105394',\n",
       " '18/01/20 20:06:33 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105398',\n",
       " '18/01/20 20:20:25 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105414',\n",
       " '18/01/20 20:32:09 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105432',\n",
       " '18/01/20 21:04:18 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105454',\n",
       " '18/01/20 21:06:18 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105462',\n",
       " '18/01/20 21:36:58 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105468',\n",
       " '18/01/20 22:06:29 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105494',\n",
       " '18/01/20 22:37:10 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105504',\n",
       " '18/01/20 23:01:18 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105524',\n",
       " '18/01/20 23:07:12 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105542',\n",
       " '18/01/20 23:23:14 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105546',\n",
       " '18/01/20 23:37:00 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105556',\n",
       " '18/01/21 00:03:09 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105578',\n",
       " '18/01/21 00:02:43 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105590',\n",
       " '18/01/21 00:01:51 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105594',\n",
       " '18/01/21 00:07:14 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105639',\n",
       " '18/01/21 00:11:01 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105651',\n",
       " '18/01/21 00:39:39 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105672',\n",
       " '18/01/21 00:17:03 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105680',\n",
       " '18/01/21 00:17:45 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105704',\n",
       " '18/01/21 00:18:15 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105712',\n",
       " '18/01/21 00:35:29 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105743',\n",
       " '18/01/21 00:37:40 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105754',\n",
       " '18/01/21 00:37:21 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105758',\n",
       " '18/01/21 00:41:23 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105800',\n",
       " '18/01/21 00:41:49 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105802',\n",
       " '18/01/21 02:13:32 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105816',\n",
       " '18/01/21 00:58:41 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105844',\n",
       " '18/01/21 00:42:51 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105851',\n",
       " '18/01/21 00:43:04 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105854',\n",
       " '18/01/21 00:54:35 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105884',\n",
       " '18/01/21 00:57:11 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105888',\n",
       " '18/01/21 01:01:55 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105928',\n",
       " '18/01/21 01:06:30 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105954',\n",
       " '18/01/21 01:08:16 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105961',\n",
       " '18/01/21 01:10:41 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105973',\n",
       " '18/01/21 01:17:10 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105979',\n",
       " '18/01/21 01:19:07 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_105989',\n",
       " '18/01/21 02:02:05 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106008',\n",
       " '18/01/21 01:22:07 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106007',\n",
       " '18/01/21 03:24:09 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106021',\n",
       " '18/01/21 02:02:33 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106052',\n",
       " '18/01/21 02:04:45 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106068',\n",
       " '18/01/21 02:07:01 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106079',\n",
       " '18/01/21 02:13:59 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106096',\n",
       " '18/01/21 02:18:27 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106111',\n",
       " '18/01/21 02:30:58 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106121',\n",
       " '18/01/21 03:05:37 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106168',\n",
       " '18/01/21 03:13:33 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106166',\n",
       " '18/01/21 02:18:57 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106164',\n",
       " '18/01/21 02:19:13 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106186',\n",
       " '18/01/21 02:28:12 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106199',\n",
       " '18/01/21 02:26:48 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106213',\n",
       " '18/01/21 03:01:51 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106251',\n",
       " '18/01/21 03:07:02 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106279',\n",
       " '18/01/21 03:10:12 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106300',\n",
       " '18/01/21 03:28:04 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106328',\n",
       " '18/01/21 03:17:26 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106336',\n",
       " '18/01/21 03:21:04 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106359',\n",
       " '18/01/21 03:20:43 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106369',\n",
       " '18/01/21 03:26:14 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106386',\n",
       " '18/01/21 03:36:56 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106418',\n",
       " '18/01/21 03:56:13 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106433',\n",
       " '18/01/21 03:59:44 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106442',\n",
       " '18/01/21 03:57:41 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106444',\n",
       " '18/01/21 04:21:49 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106454',\n",
       " '18/01/21 04:03:43 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106472',\n",
       " '18/01/21 04:03:45 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106473',\n",
       " '18/01/21 04:04:02 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1513303661803_106476']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_f.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the lines to words\n",
    "documents = data_sample.map(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['18/01/20',\n",
       "  '14:07:44',\n",
       "  'Thread-3',\n",
       "  'INFO',\n",
       "  'HadoopFsRelation:',\n",
       "  'Ignore',\n",
       "  'File',\n",
       "  'not',\n",
       "  'found',\n",
       "  'exceptions',\n",
       "  '-',\n",
       "  'true'],\n",
       " ['18/01/20',\n",
       "  '16:18:50',\n",
       "  'Thread-3',\n",
       "  'INFO',\n",
       "  'HadoopFsRelation:',\n",
       "  'Final',\n",
       "  'Fake',\n",
       "  'Statuses',\n",
       "  'count',\n",
       "  '179054',\n",
       "  'Total',\n",
       "  'time',\n",
       "  'to',\n",
       "  'get',\n",
       "  'all',\n",
       "  'the',\n",
       "  'file',\n",
       "  'statuses',\n",
       "  '-',\n",
       "  '31',\n",
       "  'secs']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "#hashingTF = HashingTF()\n",
    "#tf = hashingTF.transform(documents)\n",
    "\n",
    "# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "#tf.cache()\n",
    "#idf = IDF().fit(tf)\n",
    "#tfidf = idf.transform(tf)\n",
    "\n",
    "# spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# which occur in less than a minimum number of documents.\n",
    "# In such cases, the IDF for these terms is set to 0.\n",
    "# This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "tfidfIgnore = idfIgnore.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "\n",
    "n_clusters = 50\n",
    "# Build the model (cluster the data)\n",
    "kmeans = KMeans.train(tfidfIgnore, n_clusters, maxIterations=7, initializationMode=\"random\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the clustering labels\n",
    "kmeans_pre = kmeans.predict(tfidfIgnore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(kmeans_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine lines with clustering labels\n",
    "lines_cluster = data.zip(kmeans_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('18/02/07 16:40:49 main INFO Utils: Registered signal handlers for exception exit hook [TERM, HUP, INT]',\n",
       "  0),\n",
       " ('18/02/07 16:41:42 main INFO HadoopFsRelation: Ignore File not found exceptions - true',\n",
       "  0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_cluster.take(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
